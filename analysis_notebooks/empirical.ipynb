{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirical analyses\n",
    "\n",
    "* Loads in and joins processed+modeled brain data.\n",
    "\n",
    "* Performs boostrapping.\n",
    "\n",
    "* Creates and saves dataframes that support the figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "from glob import glob\n",
    "import nibabel as nib\n",
    "from scipy import io\n",
    "import numpy as np\n",
    "from numpy.random import SeedSequence, default_rng\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import utils\n",
    "import time\n",
    "import ipyparallel as ipp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________\n",
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing profile dir: '/Users/sfavila/.ipython/profile_default'\n",
      "Starting 4 engines with <class 'ipyparallel.cluster.launcher.LocalEngineSetLauncher'>\n"
     ]
    }
   ],
   "source": [
    "cluster = ipp.Cluster(n=4)\n",
    "cluster.start_cluster_sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 4/4 [00:06<00:00,  1.72s/engine]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc = cluster.connect_client_sync()\n",
    "rc.wait_for_engines(4); rc.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = rc[:]\n",
    "v = rc.load_balanced_view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign project paths and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = op.join('..', 'data')\n",
    "df_dir = op.join(data_dir, 'dataframes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects, rois, tasks = utils.default_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = 157245829812966997872450835235695796168 \n",
    "ss = SeedSequence(entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------\n",
    "## Data Loading "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behavior\n",
    "Load subject .csv files into dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load stim assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stim = 4\n",
    "stim_info = []\n",
    "for s_i, subj in enumerate(subjects):\n",
    "    stim_file = op.join(data_dir,  subj, 'behav', 'stim_info.csv')\n",
    "    s = pd.read_csv(stim_file).sort_values(by='stim_id').set_index('stim_id')\n",
    "    stim_info.append(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Learning*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_behav = []\n",
    "for s_i, subj in enumerate(subjects):\n",
    "    behav_files = glob(op.join(data_dir, subj, 'behav', 'learn', 'data_block??.csv'))\n",
    "    for b in behav_files:\n",
    "        df = pd.read_csv(b)\n",
    "        df['subj'] = subj\n",
    "        learn_behav.append(df)\n",
    "learn_behav = pd.concat(learn_behav, sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Scan*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_behav = []\n",
    "for s_i, subj in enumerate(subjects):\n",
    "    behav_files = glob(op.join(data_dir, subj, 'behav', 'scan', 'data_block??.csv'))\n",
    "    for b in behav_files:\n",
    "        df = pd.read_csv(b)\n",
    "        df['subj'] = subj\n",
    "        scan_behav.append(df)\n",
    "scan_behav = pd.concat(scan_behav, sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write out compiled behavioral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not op.exists(df_dir):\n",
    "    os.makedirs(df_dir)\n",
    "learn_behav.to_csv(op.join(df_dir, 'behav_learn_data.csv'), index=False)\n",
    "scan_behav.to_csv(op.join(df_dir, 'behav_scan_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brain\n",
    "\n",
    "Load .mgz and .mat file data into dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load retinotopy parameters \n",
    "\n",
    "Load lh and rh retinotopy parameters from .mgz files and combine into bilateral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_model = 'css'\n",
    "ret_params = ['full-xcrds', 'full-ycrds', 'full-eccen', 'full-angle', \n",
    "              'full-sigma', 'full-vexpl', 'full-prfbeta']\n",
    "ret = {p:[] for p in ret_params}\n",
    "\n",
    "for param, subj in itertools.product(ret_params, subjects):\n",
    "    rdir = op.join(data_dir, subj, 'retinotopy', ret_model)\n",
    "    rfile_lh = op.join(rdir, 'lh.{}.mgz').format(param)\n",
    "    r_lh = nib.load(rfile_lh).get_fdata().squeeze()\n",
    "    \n",
    "    rfile_rh = op.join(rdir, 'rh.{}.mgz').format(param)\n",
    "    r_rh = nib.load(rfile_rh).get_fdata().squeeze()\n",
    "    \n",
    "    ret[param].append(np.concatenate([r_lh, r_rh]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load GLMdenoise results\n",
    "\n",
    "Load glmdenoise params. The .mat file, which contains bilateral (concatenated) parameters is used here, but .mgz  files for the beta weights are also provided with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_df = {t: {'beta':[], 'se':[], 'R2':[], 'conds':[]} for t in tasks}\n",
    "\n",
    "for task, subj in itertools.product(tasks, subjects):\n",
    "    \n",
    "    # Load required from mat files\n",
    "    gfile = op.join(data_dir, subj, 'glmdenoise', task, 'results.mat') \n",
    "    g = io.loadmat(gfile)\n",
    "    gparams = {'beta':       g['modelmd'][0][1], \n",
    "               'se':         g['modelse'][0][1],\n",
    "               'R2':         g['R2'],\n",
    "               'conds':     [g[0][0] for g in g['conds']]}\n",
    "    \n",
    "    # Save conditions/design matrix columns\n",
    "    glm_df[task]['conds'].append(gparams['conds'])\n",
    "    \n",
    "    # For voxelwise parameters save bilateral data\n",
    "    for param in ['beta', 'se', 'R2']:\n",
    "        glm_df[task][param].append(gparams[param])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load prf predictions\n",
    "\n",
    "Load lh and rh prf predictions from .mgz files and combine lh and rh into bilateral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ret_models = ['onegaussian', 'css', 'dog']\n",
    "pred_keys = ['pred_'+m for m in pred_ret_models]\n",
    "for m in pred_keys:\n",
    "    glm_df[m] = []\n",
    "\n",
    "for mod, subj in itertools.product(pred_ret_models, subjects):\n",
    "    pdir = op.join(data_dir, subj, 'retinotopy', mod)\n",
    "    pred_files_lh = glob(op.join(pdir, 'lh.full-pred??.mgz'))\n",
    "    pred_lh = np.vstack([nib.load(p).get_fdata().squeeze() for p in pred_files_lh]).T\n",
    "    \n",
    "    pred_files_rh = glob(op.join(pdir, 'rh.full-pred??.mgz'))\n",
    "    pred_rh = np.vstack([nib.load(p).get_fdata().squeeze() for p in pred_files_rh]).T\n",
    "    \n",
    "    glm_df['pred_'+mod].append(np.concatenate([pred_lh, pred_rh]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load ROI masks\n",
    "\n",
    "Load lh and rh rois masks from .mgz files and combine lh and rh into bilateral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = {roi:[] for roi in rois}\n",
    "for roi, subj in itertools.product(rois, subjects):\n",
    "    mdir = op.join(data_dir, subj, 'rois')\n",
    "    mfile_lh = op.join(mdir, 'lh.{}_hand.mgz').format(roi)\n",
    "    m_lh = nib.load(mfile_lh).get_fdata().squeeze().astype(bool)\n",
    "    \n",
    "    mfile_rh = op.join(mdir, 'rh.{}_hand.mgz').format(roi)\n",
    "    m_rh = nib.load(mfile_rh).get_fdata().squeeze().astype(bool)\n",
    "    \n",
    "    masks[roi].append(np.concatenate([m_lh, m_rh]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restrict all ROIs by eccentricity and  variance explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eccen_min = .5\n",
    "eccen_max = 8\n",
    "vexpl_thresh = .1\n",
    "\n",
    "for roi, subj in itertools.product(rois, subjects):\n",
    "    s_i = subjects.index(subj)\n",
    "    mask = masks[roi][s_i]\n",
    "    \n",
    "    eccen = ret['full-eccen'][s_i]\n",
    "    vexpl = ret['full-vexpl'][s_i]\n",
    "\n",
    "    mask[vexpl<=vexpl_thresh] = False\n",
    "    exclude_eccen = np.logical_or(eccen<=eccen_min, eccen>eccen_max)\n",
    "    mask[exclude_eccen] = False\n",
    "    \n",
    "    masks[roi][s_i] = mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________\n",
    "## Organize experiment and retinotopy data frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment data\n",
    "\n",
    "Mask experimental brain data to just ROIs of interest and join with behavior into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_df = []\n",
    "for subj, roi, task in itertools.product(subjects, rois, tasks):\n",
    "    \n",
    "    s_i = subjects.index(subj)\n",
    "    \n",
    "    # Get roi mask\n",
    "    mask = masks[roi][s_i]\n",
    "    \n",
    "    # Get regressors from glm and corresponding stim info\n",
    "    conds = glm_df[task]['conds'][s_i]\n",
    "    sinfo = stim_info[s_i].loc[conds]\n",
    "    \n",
    "    # Get masked betas, ses, and prf model predictions\n",
    "    beta = glm_df[task]['beta'][s_i][mask, :]\n",
    "    se = glm_df[task]['se'][s_i][mask, :]\n",
    "    pred = [glm_df[m][s_i][mask, :] for m in pred_keys]\n",
    "    \n",
    "    # Split betas by condition\n",
    "    cond_beta = np.hsplit(beta, beta.shape[1])\n",
    "    cond_se = np.hsplit(se, se.shape[1])\n",
    "    cond_pred = [np.hsplit(p, p.shape[1]) for p in pred]\n",
    "    cond_all = zip(cond_beta, cond_se, *cond_pred)\n",
    "    \n",
    "    # Make dataframe\n",
    "    df = [pd.DataFrame({'subj': subj, 'task': task, 'hemi': 'b', 'roi': roi, \n",
    "                        'beta': c[0].squeeze(), \n",
    "                        'se': c[1].squeeze(), \n",
    "                         pred_keys[0]: c[2].squeeze(),\n",
    "                         pred_keys[1]: c[3].squeeze(), \n",
    "                         pred_keys[2]: c[4].squeeze(), \n",
    "                        'vert': np.where(mask.flatten()==True)[0], \n",
    "                        'stim_id': sinfo.index[c_i],\n",
    "                        'stim_angle_brain': sinfo['stim_angle_brain'][c_i],\n",
    "                        'stim_eccen': sinfo['stim_eccen'][c_i],\n",
    "                        'stim_xpos': sinfo['stim_xpos'][c_i],\n",
    "                        'stim_ypos': sinfo['stim_ypos'][c_i]})\n",
    "         for c_i, c in enumerate(cond_all)]\n",
    "\n",
    "    exp_df.append(pd.concat(df))\n",
    "exp_df = pd.concat(exp_df).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_df = exp_df[['subj', 'hemi', 'roi', 'vert', 'task', 'stim_eccen', 'stim_id', \n",
    "                 'stim_angle_brain', 'stim_xpos', 'stim_ypos', 'beta', 'se'] + pred_keys]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retinotopy data\n",
    "\n",
    "Mask retinotopy data to just ROIs of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_df = []\n",
    "for subj, roi, param in itertools.product(subjects, rois, ret_params):\n",
    "    \n",
    "    s_i = subjects.index(subj)\n",
    "    \n",
    "    # Get ret param and mask to roi\n",
    "    r = ret[param][s_i]\n",
    "    mask = masks[roi][s_i]\n",
    "    ret_data = r[mask]\n",
    "    \n",
    "    # Make dataframe\n",
    "    df = pd.DataFrame(dict(subj=subj, task=task, hemi='b', roi=roi,\n",
    "                           ret_model=ret_model, value=ret_data, param=param, \n",
    "                           vert=np.where(mask.flatten()==True)[0]))\n",
    "    ret_df.append(df)\n",
    "ret_df = pd.concat(ret_df).reset_index(drop=True)\n",
    "\n",
    "# Make params wide\n",
    "ind_cols = ['subj', 'ret_model', 'hemi', 'roi', 'vert']\n",
    "ret_df = ret_df.pivot_table(values=['value'], index=ind_cols, columns='param').reset_index()\n",
    "ret_df.columns = ind_cols + ret_df.columns.get_level_values(1)[5:].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge experiment and retinotopy data on cortical surface vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vert_data = exp_df.join(ret_df.set_index(['subj', 'hemi', 'roi', 'vert']), on=['subj', 'hemi', 'roi', 'vert'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "## Generate spatial response profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group spatial response profiles for perception and memory data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate angular distance between each pRF center and each stimulus and bin distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vert_data = utils.calc_ang_distance(vert_data, exclude_eccen=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate eccentricity bins (supplementary analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vert_data = utils.calc_eccen_bin(vert_data, exclude_angle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write out complete dataframe with all subject ret and experimental data as well as angular distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vert_data.to_csv(op.join(df_dir, 'vertex_data.csv.gz'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize angular distance data, then fit a difference of two von Mises distributions to each roi and task, and write this info out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For fitting, drop vertices excluded from both angular distance and eccen bin groups\n",
    "vert_data = vert_data.dropna(how='all', subset=['ang_dist_bin', 'eccen_bin']) \n",
    "\n",
    "norm_data = utils.norm_group(vert_data)\n",
    "norm_data.to_csv(op.join(df_dir, 'group_ang_data.csv'), index=False)\n",
    "\n",
    "params = utils.fit_diff_vonmises(norm_data, 'beta_adj')\n",
    "params.to_csv(op.join(df_dir, 'group_ang_fits.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize eccen bin data (supplementary analysis with no fits) and write this info out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_data_eccen = utils.norm_group(vert_data, xvar='eccen_bin')\n",
    "norm_data_eccen.to_csv(op.join(df_dir, 'group_eccen_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping\n",
    "\n",
    "Create bootstrapped datasets by resampling subjects with replacement (parallelized) and generate spatial response profiles for each boot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Push only the data columns we need to reduce memory usage\n",
    "core_cols = ['subj', 'hemi', 'roi', 'vert', 'task', 'stim_id', \n",
    "             'stim_angle_brain', 'full-angle', 'ang_dist_bin', \n",
    "             'eccen_bin', 'beta', 'se']\n",
    "vd = vert_data[core_cols+pred_keys]   #include prf predictions so we can resample predictions and data at once\n",
    "\n",
    "dv.push({'vert_data':vd, 'subjects':subjects})\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_data(stream):  \n",
    "    \n",
    "    import pandas\n",
    "    \n",
    "    n, rng = stream\n",
    "    \n",
    "    # Sample subject ids with replacement\n",
    "    boot_subj = rng.choice(subjects, len(subjects), replace=True)\n",
    "    \n",
    "    # Create new df with these subjects\n",
    "    boot_data = []\n",
    "    for i, s in enumerate(boot_subj):\n",
    "        df = vert_data.query(\"subj==@s\")\n",
    "        df = df.assign(subj=i+1, orig_subj=s, n_boot=n) \n",
    "        boot_data.append(df)\n",
    "    boot_data = pandas.concat(boot_data).reset_index(drop=True)\n",
    "    \n",
    "    return boot_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 500 bootstrapped datasets across engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_boots = 500\n",
    "child_seeds = ss.spawn(n_boots)\n",
    "streams = [default_rng(s) for s in child_seeds]\n",
    "\n",
    "boot_data = v.map(bootstrap_data, enumerate(streams), ordered=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize data and fit von mises (angular distance data only) to each boot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "boot_norm, boot_eccen_norm, boot_params = [], [], []\n",
    "for b in boot_data:\n",
    "    \n",
    "    # angular distance\n",
    "    n = utils.norm_group(b, group_cols=['n_boot'])\n",
    "    p = utils.fit_diff_vonmises(n, 'beta_adj', group_cols=['n_boot'])\n",
    "    boot_norm.append(n)\n",
    "    boot_params.append(p)\n",
    "    \n",
    "    # eccen bins\n",
    "    n_e = utils.norm_group(b, xvar='eccen_bin', group_cols=['n_boot'])\n",
    "    boot_eccen_norm.append(n_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save angular distance group data and params for each boot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "boot_norm = pd.concat(boot_norm).reset_index(drop=True)\n",
    "boot_norm = boot_norm.sort_values(by=['n_boot', 'roi', 'task'])\n",
    "boot_norm.to_csv(op.join(df_dir, 'group_ang_data_boots.csv'), index=False)\n",
    "\n",
    "boot_params = pd.concat(boot_params).reset_index(drop=True)\n",
    "boot_params = boot_params.sort_values(by=['n_boot', 'roi', 'task'])\n",
    "boot_params.to_csv(op.join(df_dir, 'group_ang_fits_boots.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save eccen group data for each boot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "boot_eccen_norm = pd.concat(boot_eccen_norm).reset_index(drop=True)\n",
    "boot_eccen_norm = boot_eccen_norm.sort_values(by=['n_boot', 'roi', 'task'])\n",
    "\n",
    "boot_eccen_norm.to_csv(op.join(df_dir, 'group_eccen_data_boots.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.purge_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual subject spatial response profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create average baseline-corrected response profiles for each subject and roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_df = []\n",
    "for (subj, hemi, roi, task), g in vd.groupby(['subj', 'hemi', 'roi', 'task']):\n",
    "    avg = g.groupby(['subj', 'hemi', 'roi', 'task', 'ang_dist_bin']).median().reset_index()\n",
    "    avg['beta_adj'] = avg['beta'] - avg.query(\"ang_dist_bin>=160 | ang_dist_bin<=-160\")['beta'].mean() \n",
    "    subj_df.append(avg)\n",
    "subj_df = pd.concat(subj_df).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_df = subj_df[['subj', 'hemi', 'roi', 'task', 'ang_dist_bin', 'beta_adj']]\n",
    "subj_df.to_csv(op.join(df_dir, 'subj_ang_data.csv'), index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_fits = utils.fit_diff_vonmises(subj_df, 'beta_adj', xvar='ang_dist_bin', group_cols=['subj'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_fits.to_csv(op.join(df_dir, 'subj_ang_fits.csv'), index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  pRF prediction spatial response profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize predicted data for each prf model, fit von Mises, and write this info out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_norm, pred_params = [], []\n",
    "for pred_mod in pred_keys:\n",
    "    \n",
    "    n = utils.norm_group(vert_data.query(\"task=='perception'\"), yvar=pred_mod)\n",
    "    pred_norm.append(n)\n",
    "    \n",
    "    p = utils.fit_diff_vonmises(n, pred_mod+'_adj', drop_cols='task')\n",
    "    p.loc[:, 'prf_model'] = pred_mod\n",
    "    pred_params.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_norm = pd.concat(pred_norm, sort=True)\n",
    "pred_norm = pred_norm.melt(id_vars=['hemi', 'roi', 'ang_dist_bin', 'ang_dist_bin_rad'], \n",
    "                           value_vars=['pred_onegaussian_adj', 'pred_css_adj', 'pred_dog_adj'],\n",
    "                           var_name='prf_model', value_name='beta_adj').dropna().reset_index(drop=True)\n",
    "pred_norm['prf_model'] = pred_norm['prf_model'].apply(lambda x:x[:-4])\n",
    "pred_norm.to_csv(op.join(df_dir, 'pRFpred_group_ang_data.csv'), index=False) \n",
    "\n",
    "\n",
    "pred_params = pd.concat(pred_params).reset_index(drop=True)\n",
    "pred_params.to_csv(op.join(df_dir, 'pRFpred_group_ang_fits.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize and fit von Mises to predictions in each bootstrap (boots computed in the bootstrapping section above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in multiply\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in double_scalars\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: invalid value encountered in true_divide\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in exp\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in multiply\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: invalid value encountered in true_divide\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in double_scalars\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in exp\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in multiply\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: invalid value encountered in true_divide\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in double_scalars\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in exp\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in double_scalars\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in multiply\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: invalid value encountered in true_divide\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in exp\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in multiply\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: invalid value encountered in true_divide\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in double_scalars\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in exp\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in multiply\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: invalid value encountered in true_divide\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in double_scalars\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in exp\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in multiply\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: invalid value encountered in true_divide\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in double_scalars\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in exp\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in multiply\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: invalid value encountered in true_divide\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in double_scalars\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in exp\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in multiply\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: invalid value encountered in true_divide\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in double_scalars\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in exp\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in multiply\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: invalid value encountered in true_divide\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in double_scalars\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in exp\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in multiply\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: invalid value encountered in true_divide\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in double_scalars\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in exp\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in double_scalars\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in multiply\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: invalid value encountered in true_divide\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in exp\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in multiply\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: invalid value encountered in true_divide\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in double_scalars\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in exp\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in multiply\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: invalid value encountered in true_divide\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in double_scalars\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in exp\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in multiply\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: invalid value encountered in true_divide\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in double_scalars\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in exp\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in multiply\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: invalid value encountered in true_divide\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in double_scalars\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in exp\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in multiply\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: invalid value encountered in true_divide\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in double_scalars\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n",
      "/Volumes/server/Projects/Spatial_Memory/public/osf/analysis_notebooks/utils.py:71: RuntimeWarning: overflow encountered in exp\n",
      "  p = scale * np.exp(kappa*np.cos(theta-loc))/(2*np.pi*iv(0,kappa))\n"
     ]
    }
   ],
   "source": [
    "pred_boot_norm, pred_boot_params = [], []\n",
    "for pred_mod, b in itertools.product(pred_keys, boot_data):\n",
    "    \n",
    "    n = utils.norm_group(b.query(\"task=='perception'\"), yvar=pred_mod, group_cols=['n_boot'])\n",
    "    pred_boot_norm.append(n)\n",
    "    \n",
    "    p = utils.fit_diff_vonmises(n, pred_mod+'_adj', group_cols=['n_boot'])\n",
    "    p.loc[:, 'prf_model'] = pred_mod\n",
    "    pred_boot_params.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_boot_norm = pd.concat(pred_boot_norm, sort=True)\n",
    "pred_boot_norm = pred_boot_norm.melt(id_vars=['n_boot', 'hemi', 'roi', 'ang_dist_bin'], \n",
    "                                     value_vars=['pred_onegaussian_adj', 'pred_css_adj', 'pred_dog_adj'],\n",
    "                                     var_name='prf_model', value_name='beta_adj').dropna().reset_index(drop=True)\n",
    "pred_boot_norm['prf_model'] = pred_boot_norm['prf_model'].apply(lambda x:x[:-4])\n",
    "pred_boot_norm = pred_boot_norm.sort_values(by=['n_boot', 'roi', 'prf_model'])\n",
    "pred_boot_norm.to_csv(op.join(df_dir, 'pRFpred_group_ang_data_boots.csv'), index=False)\n",
    "\n",
    "\n",
    "pred_boot_params = pd.concat(pred_boot_params).reset_index(drop=True)\n",
    "pred_boot_params = pred_boot_params.sort_values(by=['n_boot', 'roi', 'prf_model'])\n",
    "pred_boot_params.to_csv(op.join(df_dir, 'pRFpred_group_ang_fits_boots.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
